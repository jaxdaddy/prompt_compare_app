# Prompt Compare Application

This application compares two different prompts for generating financial news summaries and evaluates them based on several metrics.

## Usage

1.  Place your COR metric file (e.g., `COR_Movers_YYYY-MM-DD.pdf`), primer file (`options_primer.pdf`), and prompt set files (`prompt_set_a.txt`, `prompt_set_b.txt`) in the `files/` directory.

2.  **API Keys & Configuration:**
    *   Create a `.env` file in the project root.
    *   Add your Gemini API key: `GEMINI_API_KEY=your_gemini_api_key_here` (Get it from [Google AI Studio](https://aistudio.google.com/)).
    *   Add your NewsAPI key: `NEWSAPI_KEY=your_newsapi_key_here` (Get a free key from [https://newsapi.org/](https://newsapi.org/)).
    *   Set the debug mode: `DEBUG=True` or `DEBUG=False`.

3.  Install the required dependencies: `pip install -r requirements.txt`

4.  Run the application: `python app.py`

## Features

*   **Automatic COR File Selection:** The application automatically identifies and selects the newest `COR_Movers_YYYY-MM-DD.pdf` file from the `files/` directory for processing.

*   **News Fetching with NewsAPI:** The application uses NewsAPI exclusively to fetch recent financial news for the companies identified in the COR file.

*   **Improved Relevance Ranking:** The relevance of generated summaries is now calculated using a combination of:
    *   **LLM Relevance Score:** A qualitative score (1-10) and justification provided by the Gemini LLM.
    *   **Cosine Similarity:** A quantitative measure of semantic similarity between the summary and the news articles, calculated using `sentence-transformers`.
    *   A final combined relevance score is derived from these two metrics.

*   **Automated Summary Evaluation:** The application includes a `summary_evaluator` module that provides a detailed analysis of the generated summaries based on a set of predefined metrics. These metrics include:
    *   **Relevance:** Metric Alignment, Data Relevance, and Primer Consistency.
    *   **Readability:** Structure, Clarity, and Writing Quality.
    *   A final **Composite Score** is calculated based on a weighted average of the relevance and readability scores.

*   **Debug Mode:**
    *   When `DEBUG=True` in the `.env` file, the application will fetch news for only the first two company tickers. This allows for faster testing of the application pipeline.
    *   When `DEBUG=False`, the application will fetch news for all identified tickers.

## Database Viewer (`db_viewer.py`)

This module provides a web-based interface (using Shiny for Python) to visualize and compare the metrics generated by `app.py` and stored in `prompt_compare.db`.

### Features

*   **Tabbed Interface:** The UI is organized into two tabs for easy navigation:
    *   **Comparison View:** Displays a side-by-side comparison of all key metrics, including reading level, word count, relevance scores, and the new detailed evaluation scores from the `summary_evaluator` module.
    *   **Metrics Graph View:** Shows interactive line charts displaying the trend of selected metrics over time, including all the new evaluation scores.
*   **Run History:** View a list of all previous application runs in the sidebar.
*   **Data Refresh:** A "Refresh Data" button allows you to load the latest runs from the database without restarting the application.

### Usage

To run the database viewer, execute the following command in your terminal:

```bash
shiny run db_viewer.py
```

This will launch a local web server, and you can access the viewer in your browser (usually at `http://127.0.0.1:8000`).

## Output

Upon successful execution of `app.py`, the application will generate the following files in the `output/` directory:

*   `news_summary_mmddyyyy.txt`: Aggregated financial news content.
*   `summary_A_yyyymmdd.txt`: Summaries generated using Prompt Set A.
*   `summary_B_yyyymmdd.txt`: Summaries generated using Prompt Set B.
*   `summary_A_yyyymmdd.pdf`: Styled PDF document for Summary A.
*   `summary_B_yyyymmdd.pdf`: Styled PDF document for Summary B.
*   `output/report.txt`: The final report with the metrics from the last 5 runs.
*   `prompt_compare.db`: An SQLite database storing all run artifacts and metrics.